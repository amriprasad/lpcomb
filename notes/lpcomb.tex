\documentclass{amsbook}
\usepackage{amssymb,hyperref}
\newcommand{\vv}{\mathbf v}
\newcommand{\xx}{\mathbf x}
\newcommand{\yy}{\mathbf y}
\newcommand{\cc}{\mathbf c}
\newcommand{\bb}{\mathbf b}
\renewcommand{\aa}{\mathbf a}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\RR}{\mathbf R}
\newcommand{\MM}{\mathbf M}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{remark}[theorem]{Remark}
\begin{document}
\title{Linear Programming in Combinatorics}
\author{Amritanshu Prasad}
\address{The Institute of Mathematical Sciences, Chennai.}
\address{Homi Bhabha National Institute, Mumbai.}
\email{amri@imsc.res.in}
\date{\today}
\maketitle
\chapter{Introduction to Linear Programming}
\label{cha:intro-lp}
An excellent reference for the material in this section is the book of G\"artnet and Matousek \cite{GM}.
\section{Feasibility and Optimization}
\label{sec:feas-opt}
A \emph{linear program in equational form} consists of a set of variables $\xx=(x_1,\dotsc,x_n)$, an $m\times n$ matrix $A$ with real entries, a bound vector $\bb=(b_1,\dotsc,b_m)$, and an objective vector $\cc = (c_1,\dotsc,c_n)$.
The \emph{linear program} is the problem:
\begin{equation}
  \tag{LP}
  \label{eq:lp-problem}
  \text{maximize $\cc^T\xx$ subject to $\xx\geq 0$ and $A\xx=\bb$}.
\end{equation}
The set
\begin{displaymath}
  P(A,\bb) = \{\xx\in \RR^n\mid \xx\geq 0,\;A\xx=\bb\}
\end{displaymath}
is called the polytope of all \emph{feasible solutions} to \eqref{eq:lp-problem}.
The function $\xx\mapsto \cc^T\xx$ is called the \emph{objective function}.
An \emph{optimal solution} is a vector $\xx_0\in P(A,\bb)$ such that $\cc^T\xx\leq \cc^T\xx_0$ for every $\xx\in P(A,\bb)$.
Sometimes we will only be interested in the set $P(A,\bb)$ of feasible solutions, which does not depend on the objective vector.

Assume that $\bb$ lies in the column space of $A$ (for otherwise, we would have $P(A,\bb)=\emptyset$), and that the rows of $A$ are linearly independent (if not, we could discard redundant rows to achieve this).
For a subset $B\subset [n]$, let $A_B$ denote the submatrix of $A$ consisting of columns from $B$.
We say that $B$ is a \emph{basic set} if $B$ has $m$ elements and $A_B$ has rank $m$.
For $\xx\in \RR^n$ define:
\begin{displaymath}
  \supp(\xx)=\{1\leq j\leq n\mid x_j\neq 0\}.
\end{displaymath}
\begin{definition}
  [Basic feasible solution]
  A \emph{basic feasible solution} to \eqref{eq:lp-problem} is a feasible solution $\xx\in P(A,\bb)$ such that $\supp(\xx)$ is contained in a basic set.
\end{definition}
Clearly, a feasible solution $\xx$ is basic if and only if the submatrix $A_{\supp(\xx)}$ has linearly independent columns.
\begin{example}[Transportation polytopes]
  \label{example:transportation}
  Take $n=rs$, indexing the $rs$ variables as $\xx=(x_{ij})_{1\leq i\leq r,1\leq j\leq s}$, a rectangular array with $r$ rows and $s$ columns.

  Let $\aa\in \RR_{\geq 0}^r$, and $\bb\in \RR_{geq 0}^s$ be fixed vectors.
  As constraints, say that the row sums and column sums of $\xx$ are specified by the vectors $\aa$ and $\bb$.
  \begin{align}
    \tag{R}
    \label{eq:row-sum-cons}
    \sum_j x_{ij} &= a_i \text{ for } i=1,\dotsc,r,\\
    \tag{C}
    \label{eq:col-sum-cons}
    \sum_i x_{ij} &= b_j \text{ for } j=1,\dotsc,s.
  \end{align}
  These $r+s$ constraints are not independent--the left hand side of the sum of the row sum constraints \eqref{eq:row-sum-cons} is equal to the left hand side of the sum of the column sum constraints~\eqref{eq:col-sum-cons}, being a constraint on the sum of all entries of the matrix.
  If $\sum a_i\neq \sum b_j$, then there are no feasible solutions of this program.
  Thus we assume that the vectors $\aa$ and $\bb$ have equal sums.
  Now, removing, for instance, the last column constraint gives an $(r+s-1)\times rs$ matrix $A$ of rank $r+s-1$, which is equivalent to the original system.
  We will denote the resulting polytope of feasible solutions by $\MM_{\aa\bb}$.
  This polytope is known as the \emph{transportation polytope with margins $(\aa,\bb)$}.
  For an excellent survey on transportation polytopes, see \cite{DLK2014}.

  When $r=s=2$, the constraints can be expressed in matrix form as
  \begin{equation}
    \label{eq:birkhoff2}
    \begin{pmatrix}
      1 & 1 & 0 & 0\\
      0 & 0 & 1 & 1\\
      1 & 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
      x_{11}\\x_{12}\\x_{21}\\x_{22}
    \end{pmatrix}
    =
    \begin{pmatrix}
     a_1\\a_2\\b_1
    \end{pmatrix}.
  \end{equation}
  Every submatrix of $A$ with three columns is non-singular.
  Thus there are four possible basic sets.
\end{example}
\begin{example}[Birkhoff polytopes]
  \label{example:birkhoff}
  When $r=s$ and $\aa=\bb=(1,\dotsc,1)$, then the resulting polytope is known as the $r$th \emph{Birkhoff polytope}.
  Consider the case where $r=2$, so the constraints are given by \eqref{eq:birkhoff2} with $a_1=a_2=b_1=1$.
  It is easy to see that there are only two basic solutions, given by the permutation matrices:
  \begin{displaymath}
    \begin{pmatrix}
      x_{11} & x_{12}\\
      x_{21} & x_{22}
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & 0\\
      0 & 1
    \end{pmatrix}, \text{ or }
    \begin{pmatrix}
      x_{11} & x_{12}\\
      x_{21} & x_{22}
    \end{pmatrix}
    =
    \begin{pmatrix}
      0 & 1\\
      1 & 0
    \end{pmatrix}.
  \end{displaymath}
  Each basic feasible solution corresponds to a two possible basic subsets.
\end{example}
\begin{exercise}
  Let $B$ be a basic subset of $[r]\times [s]$ for the transportation polytope.
  By definition, $B$ has cardinality $r+s-1$.
  Given $B$, define a bipartite graph $\Gamma_B$ on the set $\{1,\dotsc,r\}\sqcup \{1',\dotsc,s'\}$ by joining $i$ to $j'$ if $(i,j)\in B$.
  Show that $\Gamma_B$ is a spanning tree for the complete bipartite graph $K_{r,s}$.
  Show that this construction gives rise to a bijection from the set of basic subsets of $[r]\times[s]$ onto the set of spanning trees of $K_{r,s}$.
\end{exercise}
\begin{exercise}
  Consider the $r$th Birkhoff polytope, $\MM_{(1^r),(1^r)}$.
  Let $\sigma\in S_r$ be a permutation and suppose $\xx$ is the permutation matrix $x_{ij}=\delta_{i\sigma(i)}$.
  Then $\supp(\xx)=\{(i,\sigma_i)\mid i\in [r]\}$.
  The first $r$ rows of the corresponding column vectors of $A$ are just the coordinate vectors of $\RR^r$.
  Therefore each permutation matrix is a basic feasible solution.
  Which spanning trees does it correspond to?
\end{exercise}
\begin{lemma}
  \label{lemma:unique-for-B}
  For each basic subset $B\subset [n]$ of \eqref{eq:lp-problem}, there exists at most one basic feasible solution $\xx$ with $\supp(\xx)\subset B$.
\end{lemma}
In the situation described in the preceding lemma, we say that \emph{$\xx$ is the basic solution corresponding to $B$}.
\begin{proof}
  Let $\xx_B$ denote the vector $(x_i)_{i\in B}$.
  The matrix $A_B$ is non-singular, so the equation $A_B\xx_B=\bb$ has at most one solution.
  Solutions $\xx$ of $A\xx=\bb$ with $\supp(\xx)\subset B$ are in bijection with solutions of $A_B\xx_B=\bb$ (set $x_j=0$ for $j\notin B$ to get $\xx$ from $\xx_B$).
  Therefore $A\xx=\bb$ also has at most one solution.
\end{proof}
\begin{remark}
  The same basic feasible solution could be obtained from different basic sets.
  For example, each basic solution for $r=2$ in Example~\ref{example:birkhoff} corresponds to two basic sets.
  Also not every basic set $B$ admits a basic feasible solution.
  For example, for the $3$rd Birkhoff polytope, $B=\{(1,1),(1,2),(1,3),(2,1),(3,1)\}$ is a basic set with no feasible solution.
\end{remark}
\begin{theorem}
  [Existence of basic optimal solutions]
  \label{theorem:existence-of-basic-solutions}
  For a linear program in equational form:
  \begin{displaymath}
    \text{maximize $\cc^t\xx$ subject to $A\xx=\bb$, $\xx\geq 0$}
  \end{displaymath}
  if there is at least one feasible solution, and the objective function is bounded above on $P(A,\bb)$, then there exists at least one optimal solution.
  Among the optimal solutions there is at least one basic solution.
\end{theorem}
\begin{proof}
  We claim that, for any feasible solution $\xx_0$, there exists a basic feasible solution $\xx$ with $\cc^T\xx\geq \cc^T\xx_0$.
  This implies that an optimal solution, if it exists, will be basic.
  Suppose $\xx$ is a feasible solution.
  Among all feasible solutions $\xx$ with $\cc^T\xx\geq \cc^T\xx_0$ choose one with support of minimal cardinality and call it $\tilde \xx$.
  We will show that $A_{\supp(\tilde\xx)}$ has linearly independent columns, so that $\tilde\xx$ is basic.
  
  Suppose this is not the case.
  Then there exists a vector $\yy\in \RR^n$ with $\supp(\yy)\subset \supp(\xx)$ such that $A\yy=0$.
  Replacing $\yy$ by $-\yy$ if necessary, assume that $\cc^T\yy\geq 0$.

  We claim that we may further assume that $\yy$ has at least one \emph{negative} coordinate.
  Suppose that all the coordinates of $\yy$ are non-negative.
  If $\cc^T\yy=0$, then we can replace $\yy$ with $-\yy$.
  If $\cc^T\yy>0$ and all coordinates of $\yy$ are positive, then $\tilde\xx+t\yy$ is a feasible solution for all $t>0$.
  The objective function $\cc^T(\tilde\xx+t\yy)$ grows unboundedly as $t$ grows, contradicting its boundedness.

  Thus $\yy$ has at least one negative coordinate, hence it is possible to choose a value $t>0$ such that $\tilde\xx+t\yy$ is a feasible solution with $\supp(\tilde\xx+t\yy)\subsetneqq \supp(\tilde\xx)$.
  Since $\cc^T\yy\geq 0$, we have $\cc^T(\tilde\xx+t\yy)\geq \cc^T\tilde\xx$ and $\supp(\tilde\xx+t\yy)$.
  This contradicts the minimality condition on the cardinality of $\supp(\tilde\xx)$.

  The set of basic feasible solutions is finite.
  The element of this set that maximizes the objective function must therefore be an optimal solution.
\end{proof}
\begin{definition}
  [Vertex]
  \label{definition:vertex}
  Let $P\subset \RR^n$ be convex closed set.
  An element $\vv\in P$ is said to be a \emph{vertex} of $P$ if there exists $\cc\in \RR^n$ such that $\cc^T\xx$ attains its maximum \emph{uniquely} at $\vv$.
\end{definition}
Theorem~\ref{theorem:existence-of-basic-solutions} says that every vertex of $P(A,\bb)$ is a basic feasible solution.
The converse is also true:
\begin{theorem}
  The basic feasible solutions to \eqref{eq:lp-problem} are precisely the vertices of $P(A,\bb)$.
\end{theorem}
\begin{proof}
  Let $B\subset [n]$ be a basic subset $\vv$ be the basic feasible solution to \eqref{eq:lp-problem} with respect to $B$.
  Define $\cc$ to be the vector with $c_j=0$ for $j\in B$, and $c_j=-1$ (or any strictly negative number) otherwise.
  Then $\cc^T\vv=0$, and by Lemma~\ref{lemma:unique-for-B}, $\cc^T\xx<0$ for every $\xx\in P(A,\bb)$ different from $\xx_0$.
\end{proof}
\begin{definition}
  [General form of a linear program]
  A more general form of a linear program involves linear inequalities and equalities.
  As before take $A$ to be an $m\times n$ matrix with real entries, $\bb\in \RR^m$, and $\cc\in \RR^n$.
  A general linear program has the form:
  \begin{equation}
    \label{eq:general-lp}
    \tag{GLP}
    \text{optimize $\cc^T\xx$ subject to }a_{i1}x_1+\dotsb + a_{in}x_n\; R_i\; b_i \text{ for }i=1,\dotsc,m,
  \end{equation}
  where $R_i$ is one of the three symbols $\leq$, $\geq$, and $=$, and the word optimize is replaced by either maximize, or minimize.
  A basic feasible solution is one that is defined by equalities in $n$ linearly independent constraints (which could be equality or inequality to begin with).
\end{definition}
\begin{example}
  [Standard equational form of the simplex]
  Consider the linear program in $n$ variables with just one linear equation:
  \begin{displaymath}
    \xx\geq 0;\; x_1+\dotsb+x_n=1.
  \end{displaymath}
  The matrix $A$ in this case has a single row, and rank one.
  The polytope $P(A,1)$ is called the standard $(n-1)$-simplex.
  Every singleton subset of $[n]$ is basic.
  The basic solution corresponding to $B=\{j\}$ is the $i$th coordinate vector $e_j$.
  Given an objective vector $\cc\in \RR^n$, the optimal solution is $e_j$ where $j$ is any of the indices for which $c_j$ is maximal among the coordinates of $\cc$.
\end{example}
\begin{example}
  The cube can be defined by the inequalities:
  \begin{displaymath}
    0\leq x_i \leq 1, \text{ for } i=1,2,3.
  \end{displaymath}
  The inequality $x_i\leq 1$ can be turned into an equality by introducing \emph{slack variables} $y_i$, and writing:
  \begin{displaymath}
    x_i\geq 0,\;y_i\geq 0,\;x_i+y_i\leq 1 \text{ for }i=1,2,3.
  \end{displaymath}
  The linear program in equational form is equivalent to the original, more general one, in the sense that there is a bijection amongst their feasible solutions that maps vertices to vertices (why?).
  What are the basic subsets? What are the basic feasible solutions?
\end{example}
\begin{exercise}
  [The simplex in terms of inequalities]
  The $n$-simplex can also be expressed in terms of inequalities:
  \begin{displaymath}
    \Delta_n = \{(x_1,\dotsc,x_n)\mid 0\leq x_1\leq x_2 \leq \dotsb x_n\leq 1\}.
  \end{displaymath}
  Rewrite this in equational form.
  Determine the basic sets and basic solutions.
\end{exercise}
\begin{exercise}
  Express the hyperoctahedron:
  \begin{displaymath}
    H_n = \{\xx\mid -1\leq x_1+\dotsb+x_n\leq 1\}
  \end{displaymath}
  in equational form.
\end{exercise}

\section{Simplex Tableaus}
\label{sec:simplex-method}

The simplex method begins with a basic set $B$ for which there exists a basic feasible solution.
Let $\bar B=[n]-B$, the complement of $B$ in $[n]$.
Using the relations imposed by $A\xx=\bb$, each of the basic variables $x_j$, $j\in B$, can be expressed in terms of the non-basic variables $x_j$, $j\in \bar B$.
Using this, the objective function can also be expressed in terms of the non-basic variables.

To do this explicitly, note that the system of equations $A\xx=\bb$ can be rearranged as:
\begin{equation}
  \label{eq:basic_from_nonbasic}
  A_B\xx_B = \bb-A_{\bar B}\xx_{\bar B}.
\end{equation}
Here $\xx_B\in\RR^m$ and $\xx_{\bar B}\in\RR^{m-n}$ are vectors whose coordinates are those coordinates of $\xx$ whose indices lie in $B$ and $\bar B$ respectively. 
Since $A_B$ is invertible, the basic variables can be expressed in terms of the non-basic ones:
\begin{displaymath}
  \xx_B = A_B^{-1}(\bb-A_{\bar B}\xx_{\bar B}).
\end{displaymath}
Indeed, the basic feasible solution is computed by setting $\xx_{\bar B}=0$ in the above equation.
If $A_B^{-1}\bb\geq 0$, then it is the basic feasible solution corresponding to $B$.
Othewise there is no basic feasible solution corresponding to $B$.
Since each basic variable is expressed in terms of the non-basic variables in \eqref{eq:basic_from_nonbasic}, the objective function can be expressed in terms of the non-basic variables only:
\begin{displaymath}
  \cc^T\xx = \cc_B^T\xx_B + \cc_{\bar B}^T\xx_{\bar B} = \cc^T_BA_B^{-1}(\bb-A_{\bar B}\xx_{\bar B}) + \cc^T_{\bar B}\xx_{\bar B}.
\end{displaymath}

Thus \eqref{eq:lp-problem} is represented in terms of a \emph{simplex tableau}:
\begin{equation}
  \tag{T}
  \label{eq:tableau}
  \begin{matrix}
    \xx_B & = & \mathbf d - D\xx_{\bar B}\\
    \hline
    \cc^T\xx & = & e - \mathbf e^T \xx_{\bar B},
  \end{matrix}
\end{equation}
the equations above the line being the constraints, and the objective function $\cc^T\xx$ below the line to be maximized.
Here:
\begin{align*}
  \mathbf d & = A_B^{-1}\bb\\
  D & = A_B^{-1}A_{\bar B}\\
  e & = \cc_B^T \mathbf d\\
  \mathbf e^T & = \cc_B^TD - \cc_{\bar B}^T.
\end{align*}
The information contained in \eqref{eq:tableau} is equivalent to the information in \eqref{eq:lp-problem}.
But \eqref{eq:tableau} gives a parametrization of $P(A,b)$ in terms of a subset of $\RR^{n-m}$.
\begin{theorem}
  \label{theorem:basic-parametrization}
  For every basic subset $B\subset [n]$ in the linear program \eqref{eq:lp-problem},
  \begin{equation}
    \tag{$*$}
    \label{equation:tableau-par}
    P(A,\bb) = \{\xx\in \RR^n\mid \xx_B=A_B^{-1}(\bb-A_{\bar B}\xx_{\bar B})\geq 0,\;\xx_{\bar B}\geq 0\}.
  \end{equation}
  In other words, \eqref{eq:tableau} gives a parametrization of $P(A,\bb)$ in terms of the polytope:
  \begin{displaymath}
    P_B(A,\bb) = \{\xx_{\bar B}\in \RR_{\geq 0}^{n-m}\mid A_B^{-1}(\bb-A_{\bar B}\xx_{\bar B})\geq 0\}\subset \RR^{n-m}.
  \end{displaymath}
\end{theorem}
\begin{proof}
  The conditions $A\xx=\bb$ and $\xx_B=A_B^{-1}(\bb-A_{\bar\xx}x_{\bar B})$ are equivalent.
\end{proof}
\begin{example}
  \label{example:birkhoff3-tableau}
  Consider the Birkhoff polytope (Example~\ref{example:birkhoff}) for $r=3$.
  For convenience we abbreviate the variable indices $(i,j)$ to $ij$.
  The matrix $A$ has columns indexed by pairs in $[d]\times [d]$ written in increasing lexicographic order:
  \begin{displaymath}
    A =
    \begin{pmatrix}
      1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
      0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0\\
      0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\
      1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\
      0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\\
    \end{pmatrix}
  \end{displaymath}
  Fix as objective function $x_{11}+x_{22}+x_{33}$.
  A basic subset is $B=\{11,12,13,22,31\}$ with basic solution given by $x_{13}=x_{22}=x_{31}=1$, and all other coordinates zero.
  The corresponding simplex tableau is:
  \begin{displaymath}
    \begin{matrix}
      x_{11} & = & -x_{21} +x_{32}+x_{33}\\
      x_{12} & = & x_{21}+x_{23}-x_{32}\\
      x_{13} & = & 1-x_{23}-x_{33}\\
      x_{22} & = & 1-x_{21} -x_{23}\\
      x_{31} & = & 1-x_{32}-x_{33}\\
      \hline
      \cc^T\xx & = & 1-2x_{21}-x_{23}+x_{32}+2x_{33}.
    \end{matrix}
  \end{displaymath}
  In the above example, the objective function can be increased by increasing $x_{32}$ or $x_{33}$, the variables with positive coefficients in the last row of the tableau.
  However, this increase should respect the constraints that all the variables are non-negative.
  The condition $x_{12}\geq 0$ (using the second equation, and leaving the values of $x_{21}$ and $x_{23}$ unchanged at $0$) gives $x_{32}\leq 0$.
  Therefore, it is not feasible to increase $x_{32}$.
  However, it is feasible to increase $x_{33}$.
  The conditions $x_{13}\geq 0$ and $x_{31}\geq 0$ give $x_{33}\leq 1$.
  So we set $x_{33}=1$, and recalculate all the basic variables, getting $x_{11}=x_{22}=x_{33}=1$, and all other variables $0$.
  We move $x_{33}$ to the set of basic variables, and move $x_{13}$ to the set of non-basic (which has now become $0$), and use the equation:
  \begin{displaymath}
    x_{33}=1-x_{13}-x_{23}.
  \end{displaymath}
  Using this we get a new tableau:
  \begin{displaymath}
    \begin{matrix}
      x_{11} & = & 1-x_{21} +x_{32}+x_{23}-x_{13}\\
      x_{12} & = & x_{21}+x_{23}-x_{32}\\
      x_{22} & = & 1-x_{21} -x_{23}\\
      x_{31} & = & x_{13}+x_{23}-x_{32}-x_{33}\\
      x_{33} & = & 1-x_{23}-x_{13}\\
      \hline
      \cc^T\xx & = & 3-2x_{13}-2x_{21}-3x_{23}+x_{32}.
    \end{matrix}
  \end{displaymath}
  All the non-basic variables have negative coefficients, except $x_{32}$.
  However, the constraint $x_{12}\geq 0$ still does not allow us to increase $x_{32}$ without changing any other non-basic variable.
  This suggests that we may have arrived at a maximum value for the objective function.
  Indeed, $x_{12}=x_{21}+x_{23}-x_{32}\geq 0$ implies that $x_{32}\leq x_{21}+x_{23}$ for every $\xx\in P(A,\bb)$, whence
  \begin{displaymath}
    \cc^T\xx=3-2x_{13}-2x_{21}-3x_{23}+x_{32}\leq 3-2x_{13}-x_{21}-2x_{23}\leq 3.
  \end{displaymath}
  Therefore $3$ is indeed a global maximum for the objective function, and is obtained uniquely at $x_{ij}=\delta_{ij}$.

  An alternative approach would be to induct $x_{32}$ into the set of basic variables, and remove $x_{12}$.
  Now the basic set is changed to $\{11,22,31,32,33\}$, but the basic feasible solution remains unchanged.
  This will result in the tableau:
  \begin{displaymath}
    \begin{matrix}
      x_{11} & = & 1-x_{12}+x_{32}+2x_{23}-x_{13}\\
      x_{22} & = & 1-x_{21} -x_{23}\\
      x_{31} & = & x_{12}+x_{13}-x_{21}-x_{33}\\
      x_{32} & = & x_{21}+x_{23}-x_{12}\\
      x_{33} & = & 1-x_{23}-x_{13}\\
      \hline
      \cc^T\xx & = & 3-2x_{13}-x_{21}-2x_{23}-x_{12}.
    \end{matrix}
  \end{displaymath}
  In this case, all the coefficients of the objective function, when expressed in terms of non-basic variables, are negative.
  Theorem~\ref{theorem:basic-parametrization} then implies that $x_{ij}=\delta_{ij}$ is the unique global maximum.
\end{example}
More generally Theorem~\ref{theorem:basic-parametrization} gives:
\begin{theorem}
  \label{theorem:pivot-basic}
  Let $B$ be a basic set for \eqref{eq:lp-problem}.
  Suppose that the objective function, when expressed in terms of the non-basic variables as in the last line of the tableau \eqref{eq:tableau} has all coefficients nonpositive.
  Then the basic feasible solution $\xx_0$ corresponding to $B$ is a solution to \eqref{eq:lp-problem}.
  If all the non-basic variables occur with strictly negative coefficients, then $\xx_0$ is the unique solution to \eqref{eq:lp-problem}.
\end{theorem}

\section{Pivot Rules}
\label{sec:pivot-rules}

Given a basic subset $B$ for the linear program \eqref{eq:lp-problem}, consider the corresponding tableau \eqref{eq:tableau}.
Assume also that $B$ admits a basic feasible solution $\xx$.
Suppose that $e_k>0$ for some $k\in \bar B$.
For each $i$ in $B$, we have:
\begin{equation}
  \label{eq:bound-i}
  x_i = d_i - \sum_{j\in \bar B} D_{ij}x_j.
\end{equation}
The constraint $x_i\geq 0$ can be rewritten as:
\begin{displaymath}
  D_{ik}x_k \leq d_i - \sum_{j\in \bar B,\;j\neq k} D_{ij}x_j.
\end{displaymath}
Since $B$ admits a basic feasible solution where all the basic variables are zero, we have $d_i\geq 0$.
If $D_{ik}\leq 0$, then $x_k$ can be increased without bound without violating \eqref{eq:bound-i}.
If, on the other hand, $D_{ik}>0$, then $x_k$ can be increased up to $d_i/D_{ik}$.
Therefore, we can increase $x_k$ up to $m=\min\{d_i/D_{ik}\mid i\in B, D_{ik}>0\}$.
In general $m\geq 0$, and it is quite possible (as we have seen in Example~\ref{example:birkhoff3-tableau}) that $m=0$.
Let $l$ be any index for which $m=d_l/D_{il}$.
Let $B'=B\cup\{k\}-\{l\}$.

We have an expression for $x_k$ in terms of the variables $\{x_j\mid j\in \bar B'\}$:
\begin{equation}
  \label{eq:pivot-sub}
  x_k = D_{lk}^{-1}(d_l-x_l-\sum_{j\in \bar B\;j\neq k} D_{lj}x_j).
\end{equation}
This can be substituted into the equations \eqref{eq:bound-i} for all $i\in B-\{l\}$ to write $x_i$ in terms of the variables $\{x_j\mid j\in \bar B'\}$.
Thus each variable $x_i$, for $i\in B'$ can be expressed in terms of the variables $\{x_j\mid j\in \bar B'\}$.
Also, the substitution \eqref{eq:pivot-sub} can be used to express the objective function in terms of the variables $\{x_j\mid j\in \bar B'\}$.
Let $\xx'$ be the vector obtained from $\xx$ by changing $x_k$ to $m$, and $x_l$ to $0$.
\begin{lemma}
  If $B$ is a basic subset with basic feasible vector $\xx$ and $B'$ and $\xx'$ are obtained from $B$ and $\xx$ as described above, then $B'$ is also a basic subset, and $\xx'$ is a basic feasible solution corresponding to $B'$.
\end{lemma}
\begin{proof}
  Since $B$ is a basic subset, the analysis at the beginning of Section~\ref{sec:simplex-method} shows that the equations:
  \begin{displaymath}
    A\xx=\bb \text{ and } \xx_B = \mathbf d - D\bar \xx_{\bar B} 
  \end{displaymath}
  are equivalent.
  Our choices of $k$ and $l$ above ensure that these equations are further equivalent to
  \begin{displaymath}
    \xx_{B'} = \mathbf d' - D'\bar \xx_{\bar B'},
  \end{displaymath}
  for some vector $\mathbf d'$ and some matrix $D'$.
  Setting $\xx_{\bar B'}=0$ says that there is a unique value for $\xx_{B'}$ such that, if $\xx''\in \RR^n$ is obtained from $\xx_{B'}$ by setting the coordinates in $\bar B'$ to $0$, then $A\xx''=\bb$.
  In other words, $A_{B'}\xx_{B'}=\bb$ has a unique solution.
  It follows that $A_{B'}$ is non-singular, and so $B'$ basic.
  By Lemma~\ref{lemma:unique-for-B}, the vector $\xx''$ constructed in this proof coincides with the vector $\xx'$ constructed earlier, so the rest of the lemma also follows.
\end{proof}
\begin{definition}
  [Pivot step]
  \label{definition:pivot}
  Given a basic subset $B\subset[n]$ and the corresponding tableau \eqref{eq:tableau} for a linear program \eqref{eq:lp-problem}, a \emph{leaving variable} is any variable $x_k$ such that $k\in \bar B$, and $e_k>0$.
  Let $m=\min\{d_i/D_{ik}\mid D_{ik}>0\}$.
  Assume that $m<\infty$ (we will come to the case $m=\infty$ shortly).
  An \emph{entering variable} is any variable $x_l$ such that $m=d_l/D_{lk}$.
  A pivot step is a change of basic set $B\mapsto B':=B\cup\{x_l\}-\{x_k\}$, where $x_l$ is an entering variable and $x_k$ is a leaving variable.
  By Theorem~\ref{theorem:pivot-basic}, $B'$ is again a basic subset for \eqref{eq:lp-problem}.
  If $\xx$ is the basic feasible solution corresponding $B$, and $\xx'$ is the basic feasible solution corresponding to $B'$, then $\cc^T\xx'\geq \cc^T\xx$.
\end{definition}
In what situation is there no possible pivot step?
The first possibility is that $e_k\leq 0$ for all $k\in \bar B$.
Then Theorem~\ref{theorem:pivot-basic} tells us that the basic feasible solution corresponding to $B$ is maximizes the objective function of $P(A,\bb)$.
Another possibility is that, for some $k\in\bar B$ such that $e_k>0$, $D_{ik}<0$ for all $i\in B$.
Then, starting with the basic feasible solution corresponding to $B$, $x_k$ can be increased unboundedly without disturbing any of the other non-basic variables (which are set to $0$) without making any of the variable $x_i$, $i\in B$ negative.
This shows that the objective function $\cc^T\xx$ is unbounded on $P(A,\bb)$.
If neither of these cases occur, then a pivot step $B\mapsto B'$ is possible.
However, it is quite possible that $m=0$ (this happened with the variable $x_{32}$ in Example~\ref{example:birkhoff3-tableau}).
In this case the pivot step $B\mapsto B'$ happens, but $\xx'=\xx$.
Of course, the value of the objective function remains unchanged.
Also, $\supp(\xx)=\sup(\xx')\subset B\cap B'\subsetneq B$.
\begin{definition}
  [Degenerate basic subset]
  A basic subset for \eqref{eq:lp-problem} is said to be \emph{degenerate} if it admits a basic feasible solution $\xx$ such that $\supp(\xx)\subsetneq B$.
\end{definition}
\begin{lemma}
  Given an $m\times n$ matrix $A$, the set of constraint vectors $\bb\in \RR^n$ for which there exist degenerate basic subsets is a finite union of hyperplanes.
\end{lemma}
\begin{proof}
  If $B\subset [n]$ is degenerate, then the vector $A_B^{-1}\bb$ lies in the union of coordinate hyperplanes.
  In other words, $\bb$ lies in image under $A_B$ of a coordinate hyperplane, which, by the non-singularity of $A_B$, is a hyperplane.
  Thus the set of constraint vectors $\bb$ for which there exists a degenerate basic subset is a finite union of a hyperplanes.
\end{proof}
Since a finite union of coordinate hyperplanes is a nowhere dense subset of $\RR^n$, it is very unlikely that degeneracy will occur in a real-world linear programming problem.
Moreover, if it does occur, a small enough perturbation of $\bb$ will bring us back to the non-degenerate world while giving the correct basic subset whose basic feasible solution also optimizes the original problem.
\begin{definition}[Non-degerate linear program]
  We say that \eqref{eq:lp-problem} is non-degenerate if it admits no degenerate basic subset $B\subset[n]$.
\end{definition}
\begin{theorem}
  Assume that \eqref{eq:lp-problem} is a non-degenerate linear program.
  Starting with any basic set $B$, a finite number of pivot steps (Definition~\ref{definition:pivot}) will result in a basic set whose basic feasible solution is optimal, or else establish that the objective function is unbounded on $P(A,\bb)$.
\end{theorem}
\begin{proof}
  Indeed, non-degeneracy ensures that the objective function increases strictly at each pivot step or $m=\infty$.
  If $m=\infty$ at any pivot step, then the objective function is unbounded.
  Otherwise, becuase the objective function increases strictly, no basic subset is ever repeated in a run of the algorithm.
  Since there are only finitely many possible basic subsets, the algorithm must halt at an optimal solution.
\end{proof}
The number of basic subsets can grow exponentially with the number of variable.
In an implementation of a linear programming algorithm a rule for choosing the leaving variable and the entering variable has to be specified.
Such a rule is called a \emph{pivot rule}.
For a discussion of commonly used pivot rules, their relative merits and demerits, the reader is referred to \cite[Section~5.7]{GM}.

However, degeneracy is ubiquitous in combinatorial applications of linear programming.
For example, \emph{every} basic subset for the $r$th Birkhoff polytope for every $r>1$ either does not admit a basic feasible solution, or is degenerate, since every vertex of the Birkhoff polytope is a permutation matrix.
Moreover, the support of any permutation matrix has $r$ elements, but every basic subset has cardinality $2r-1$.
\begin{example}
  Consider the Birkhoff polytope for $r=3$ with objective function:
  \begin{displaymath}
    x_{12}+x_{13}+x_{21}+x_{23}+x_{31}+x_{32}.
  \end{displaymath}
  For $B=\{11,12,13,22,33\}$, the tableau is
  \begin{displaymath}
    \begin{matrix}
      x_{11}&=&1-x_{21}-x_{31}\\
      x_{12}&=&x_{21}+x_{23}-x_{32}\\
      x_{13}&=&-x_{23}+x_{31}+x_{32}\\
      x_{22}&=&1-x_{21}-x_{23}\\
      x_{33}&=&1-x_{31}-x_{32}\\
      \hline
      \cc^T\xx&=&2x_{21}+x_{23}+2x_{31}+x_{32}.
    \end{matrix}
  \end{displaymath}
  All four non-basic variables have positive coefficients in the objective function.
  Using $x_{23}$ or $x_{32}$ as entering variables leads to pivots that do not increase the objective function.
\end{example}

The simplex method begins with a basic subset $B\subset [n]$ for \eqref{eq:lp-problem}.
How does one find a basic subset to begin with?
Indeed, the total number of subsets of $[n]$ is $2^n$, and finding a basic subset could be like finding a needle in a (albeit finite) haystack.
The trick is to start with an infeasible point (usually $\xx=0$), quantify its ``infeasibility'' and then move to a feasible point (if it exists) by taking the negative of this infeasibility as an objective function.

So, introduce variables $x_{n+1},\dotsc,x_{n+m}$, all constrained to take non-negative values.
The variable $x_{n+i}$ will measure the feasibility gap in the $i$th equation:
\begin{displaymath}
  a_{i1}x_1+\dotsb+a_{in}x_n+\mathrm{sgn}(b_i)x_{n+i}=b_i,
\end{displaymath}
and then maximize $-x_{n+1}-x_{n+1}-\dotsb-x_{n+m}$.
In other words, the auxilliary linear program has coefficient matrix $\tilde A = \begin{pmatrix}A & I_m\end{pmatrix}$ in block form, where $I_m$ denotes the $m\times m$ identity matrix.
The bound vector $\bb$ remains unchanged.
The objective vector is $(0,\dotsc,0,-1,\dotsc,-1)$ (with $n$ zeros and $m$ $-1$'s).
In this case $B=\{n+1,\dotsc,n+m\}$ is clearly a basic subset with basic feasible solution $\tilde\xx=(0,\dotsc,0,|b_1|,\dotsc,|b_m|)$.
The original program \eqref{eq:lp-problem} has a feasible solution if and only if the maximum value of the objective function is $0$.
Thus locating a basic subset is itself a linear programming problem with an obvious basic subset.

\section{Dealing with degeneracy}
\label{sec:deal-with-degen}

In the non-degenrate case, we saw that, with an sequence of arbitrarily chosen pivot moves, it is possible to go from any basic feasible solution to an  optimal one.
In the degenerate case, it is quite possible that we end up repeating a cycle of pivot moves indefinitely.
This can be avoided using \emph{Bland's rule} for choosing the leaving and entering variable.
\begin{definition}
  [Bland's rule]
  Given a basic subset $B\subset [n]$ for \eqref{eq:lp-problem}, choose the incoming variable with minimum possible index, and among all the outgoing variables detrmined by this incoming variable, choose the one with minimum possible index.
\end{definition}
\begin{theorem}
  With Bland's rule, starting with a given basic subset $B\subset [n]$ for \eqref{eq:lp-problem}, it is not possible to return to $B$ after a any number of pivot steps.
\end{theorem}
\begin{proof}
  
\end{proof}
\section{Duality}
\label{sec:duality}

Given a linear program \eqref{eq:lp-problem}, choose any set of non-negative constants $y_1,\dotsc,y_m$, and consider the sum of the $m$ equations represented by $A\xx=\bb$ and add them up, weighted by the constants $y_1,\dotsc,y_m$.
The result is a single linear equation in the variables $x_1,\dotsc,x_n$:
\begin{displaymath}
  \yy^TA\xx=\yy^T\bb.
\end{displaymath}
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}
